{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup and important aspects:\n",
    "- Gemma 2B and 7B \n",
    "- HuggingFace\n",
    "- LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xuananh/anaconda3/envs/code/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-12 21:14:34.084201: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-12 21:14:34.566064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/xuananh/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from accelerate.utils import release_memory\n",
    "import torch \n",
    "# from kaggle_secrets import UserSecretClient \n",
    "# from huggingface_hub import login \n",
    "import huggingface_hub\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftModel\n",
    "import pandas as pd \n",
    "import langchain \n",
    "from langchain.text_splitter import CharacterTextSplitter, HTMLHeaderTextSplitter\n",
    "from langchain.docstore.document import Document \n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "import evaluate \n",
    "import transformers\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Any \n",
    "import warnings \n",
    "import gc \n",
    "import random \n",
    "import numpy as np \n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "# Get the secret key\n",
    "huggingface_key = os.getenv(\"HUGGINGFACE_SECRET_KEY\")\n",
    "\n",
    "\n",
    "# Set seed\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "# read writeups dataset \n",
    "writeups = pd.read_csv(\"kaggle-winning-solutions-methods/kaggle_winning_solutions_methods.csv\")\n",
    "writeups = writeups.drop_duplicates(subset = ['link', 'writeup']).reset_index(drop = True)\n",
    "\n",
    "huggingface_hub.login(token=huggingface_key)\n",
    "\n",
    "model_huggingface_path = \"./google_model/gemma_2b_it\"\n",
    "pipe = transformers.pipeline('text-generation', \n",
    "                             model=model_huggingface_path,\n",
    "                              model_kwargs={\"torch_dtype\": torch.float16},\n",
    "                            device='cuda',\n",
    "                            max_new_tokens=512\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline provide an efficient and user-friendly way to leverage models for inference. They consist of:\n",
    "- A tokenizer, which, if not explicitly specified, is automatically imported from the model configurations  on HuggingFace.\n",
    "- The model itself\n",
    "- Parameters for controlling and fine-tuning the output\n",
    "\n",
    "Considering the code above ,several crucial parameters have been configured: \n",
    "\n",
    "- `max_new_tokens` - controls the **maximum number of newly generated tokens**. If not specified, the default value may not be sufficient to generate enough text (therefore summaries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 9864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<h2>TLDR</h2>\\n<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.</p>\\n<p>We used only competition data.</p>\\n<h2>1. Data Preprocessing</h2>\\n<h3>1.1 CNN Preprocessing</h3>\\n<ul>\\n<li>We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points.</li>\\n<li>During training, we applied various augmentations.</li>\\n<li>We implemented standard normalization.</li>\\n<li>Instead of dropping NaN values, we filled them with zeros after normalization.</li>\\n<li>We interpolated the time axis to a siz'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the first writeup from the dataset and inspect the first 1000 chars\n",
    "writeup_small = writeups.iloc[0, 9] # dòng số 0 và ô thứ  9 \n",
    "print('Number of characters:', len(writeup_small))\n",
    "writeup_small[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>place</th>\n",
       "      <th>competition_name</th>\n",
       "      <th>prize</th>\n",
       "      <th>team</th>\n",
       "      <th>kind</th>\n",
       "      <th>metric</th>\n",
       "      <th>year</th>\n",
       "      <th>nm</th>\n",
       "      <th>writeup</th>\n",
       "      <th>num_tokens</th>\n",
       "      <th>methods</th>\n",
       "      <th>cleaned_methods</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n",
       "      <td>2</td>\n",
       "      <td>Google - Isolated Sign Language Recognition</td>\n",
       "      <td>$100,000</td>\n",
       "      <td>1,165</td>\n",
       "      <td>Research</td>\n",
       "      <td>PostProcessorKernelDesc</td>\n",
       "      <td>2023</td>\n",
       "      <td>406306</td>\n",
       "      <td>&lt;h2&gt;TLDR&lt;/h2&gt;\\n&lt;p&gt;We used an approach similar ...</td>\n",
       "      <td>2914</td>\n",
       "      <td>['EfficientNet-B0', 'Data Augmentation', 'Norm...</td>\n",
       "      <td>Replace augmentation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n",
       "      <td>3</td>\n",
       "      <td>Google - Isolated Sign Language Recognition</td>\n",
       "      <td>$100,000</td>\n",
       "      <td>1,165</td>\n",
       "      <td>Research</td>\n",
       "      <td>PostProcessorKernelDesc</td>\n",
       "      <td>2023</td>\n",
       "      <td>406568</td>\n",
       "      <td>&lt;p&gt;We used an &lt;strong&gt;ensemble of six conv1d m...</td>\n",
       "      <td>1744</td>\n",
       "      <td>['Conv1D', 'Transformers', 'Data preprocessing...</td>\n",
       "      <td>Conv1D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.kaggle.com/c/asl-signs/discussion/...</td>\n",
       "      <td>4</td>\n",
       "      <td>Google - Isolated Sign Language Recognition</td>\n",
       "      <td>$100,000</td>\n",
       "      <td>1,165</td>\n",
       "      <td>Research</td>\n",
       "      <td>PostProcessorKernelDesc</td>\n",
       "      <td>2023</td>\n",
       "      <td>406673</td>\n",
       "      <td>&lt;p&gt;I would like to thank the organizers and al...</td>\n",
       "      <td>2189</td>\n",
       "      <td>['XY coordinates', 'Normalization', 'Flip', '1...</td>\n",
       "      <td>Max pooling</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                link  place  \\\n",
       "0  https://www.kaggle.com/c/asl-signs/discussion/...      2   \n",
       "1  https://www.kaggle.com/c/asl-signs/discussion/...      3   \n",
       "2  https://www.kaggle.com/c/asl-signs/discussion/...      4   \n",
       "\n",
       "                              competition_name     prize   team      kind  \\\n",
       "0  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n",
       "1  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n",
       "2  Google - Isolated Sign Language Recognition  $100,000  1,165  Research   \n",
       "\n",
       "                    metric  year      nm  \\\n",
       "0  PostProcessorKernelDesc  2023  406306   \n",
       "1  PostProcessorKernelDesc  2023  406568   \n",
       "2  PostProcessorKernelDesc  2023  406673   \n",
       "\n",
       "                                             writeup  num_tokens  \\\n",
       "0  <h2>TLDR</h2>\\n<p>We used an approach similar ...        2914   \n",
       "1  <p>We used an <strong>ensemble of six conv1d m...        1744   \n",
       "2  <p>I would like to thank the organizers and al...        2189   \n",
       "\n",
       "                                             methods       cleaned_methods  \n",
       "0  ['EfficientNet-B0', 'Data Augmentation', 'Norm...  Replace augmentation  \n",
       "1  ['Conv1D', 'Transformers', 'Data preprocessing...                Conv1D  \n",
       "2  ['XY coordinates', 'Normalization', 'Flip', '1...           Max pooling  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writeups.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a summary of the text in a technical way:\n",
      "\n",
      "**1. Data Preprocessing**\n",
      "\n",
      "* Extract 80 points from the image, including lip and pose points.\n",
      "* Apply various augmentations and normalizations.\n",
      "* Fill NaN values with zeros and use nearest interpolation for the time axis.\n",
      "\n",
      "**2. Augmentation**\n",
      "\n",
      "* Use common and CNN specific augmentations.\n",
      "* Implement a mixup augmentation that only works with CNNs.\n",
      "\n",
      "**3. Training**\n",
      "\n",
      "* Train EfficientNet-B0 and BERT on a single fold with 0.1 warm-up.\n",
      "* Train a transformer model with a ranger optimizer and 4-layer transformer.\n",
      "* Tune hyperparameters with Optuna.\n",
      "\n",
      "**4. Submissions**\n",
      "\n",
      "* Aggregate models in a tf.Module.\n",
      "* Calculate ensemble weights for fold 0 and apply to the full dataset.\n",
      "\n",
      "**5. PS. Need BETTER TFlite DepthwiseConv2D**\n",
      "\n",
      "* Explore different ways to implement depthwise convolution in tflite.\n",
      "* Experiment with different FLOP configurations.\n",
      "\n",
      "**6. Conclusion**\n",
      "\n",
      "* EfficientNet-B0 achieved a leaderboard score of 0.8.\n",
      "* Transformers improved the score to 0.81.\n",
      "* Ensemble of models with different architectures achieved the highest score of 0.82.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\\n\\n{}\".format(writeup_small)\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(\n",
    "    prompt,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_k=20,\n",
    "    top_p=0.3,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check a test message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "This is a test<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Good for you!<end_of_turn>\n",
      "<start_of_turn>user\n",
      "Ah ah<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"This is a test\"},\n",
    "    {\"role\": \"assistant\",\n",
    "     \"content\": \"Good for you!\"},\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"Ah ah\"},\n",
    "]\n",
    "\n",
    "test_prompt = pipe.tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(test_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'writeup_small' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m messages_eli5 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 3\u001b[0m      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummarize the following text while avoiding difficult jargon using bullet points chapters. Explain it like I am a 5 years old:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mwriteup_small\u001b[49m)},\n\u001b[1;32m      4\u001b[0m ]\n\u001b[1;32m      6\u001b[0m prompt_eli5 \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mapply_chat_template(messages_eli5, tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m outputs_eli5 \u001b[38;5;241m=\u001b[39m pipe(\n\u001b[1;32m      8\u001b[0m     prompt_eli5,\n\u001b[1;32m      9\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'writeup_small' is not defined"
     ]
    }
   ],
   "source": [
    "messages_eli5 = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"Summarize the following text while avoiding difficult jargon using bullet points chapters. Explain it like I am a 5 years old:\\n\\n{}\".format(writeup_small)},\n",
    "]\n",
    "\n",
    "prompt_eli5 = pipe.tokenizer.apply_chat_template(messages_eli5, tokenize=False, add_generation_prompt=True)\n",
    "outputs_eli5 = pipe(\n",
    "    prompt_eli5,\n",
    "    add_special_tokens=True,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_k=20,\n",
    "    top_p=0.3\n",
    ")\n",
    "print(outputs_eli5[0][\"generated_text\"][len(prompt_eli5):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTIMENT: Positive.\n",
      "SUBJECT: Notebook<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "messages_few_shot = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"This film was great, rich of details and with great actors.\"},\n",
    "    {\"role\": \"assistant\",\n",
    "     \"content\": \"SENTIMENT: Positive.\\nSUBJECT: Film\"},\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"This park is dirty.\"},\n",
    "    {\"role\": \"assistant\",\n",
    "     \"content\": \"SENTIMENT: Negative.\\nSUBJECT: Park\"},\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": \"This notebook is fantastic. I'm learning a lot\"},\n",
    "]\n",
    "\n",
    "prompt_few_shot = pipe.tokenizer.apply_chat_template(messages_few_shot, tokenize=False, add_generation_prompt=True)\n",
    "outputs_few_shot = pipe(\n",
    "    prompt_few_shot,\n",
    "    add_special_tokens=True,\n",
    "    do_sample=True,\n",
    "    temperature=0.1,\n",
    "    top_k=20,\n",
    "    top_p=0.3\n",
    ")\n",
    "print(outputs_few_shot[0][\"generated_text\"][len(prompt_few_shot):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "This film was great, rich of details and with great actors.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "SENTIMENT: Positive.\n",
      "SUBJECT: Film<end_of_turn>\n",
      "<start_of_turn>user\n",
      "This park is dirty.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "SENTIMENT: Negative.\n",
      "SUBJECT: Park<end_of_turn>\n",
      "<start_of_turn>user\n",
      "This notebook is fantastic. I'm learning a lot<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_few_shot = pipe.tokenizer.apply_chat_template(messages_few_shot, \n",
    "                                                     tokenize=False, \n",
    "                                                     add_generation_prompt=False)\n",
    "\n",
    "print(prompt_few_shot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters on pipeline transformers:   \n",
    "- `do sample`: If set to True, the model will do sampling to generate the next token. If set to False, the model will use greedy decoding.  this parameter enables decoding strategies to select the next token from the probability distribution over the entire vocabulary. Together with num_beams, we can control different strategies. I opted for True and num_beams=1 (default), which is the multinomial sampling. More of decoding strategies here and here.  \n",
    "- `temperature`: this paramater control randomness. The lower the temperature, the more deterministic the results are in the sense that the highest probable token is picked. I opted for a very low value because we need to encourage more factual responses and not creative ones.  \n",
    "- `top_p`: controls the sampling of tokens. higher values will allow more tokens to be sampled. , including less likely ones. We opted for a relatively low value to maintain coherence given the task at hand.  \n",
    "- `top_k` : in simple terms, together with top_p, it controls the number of tokens to keep for prediction. Once again, a low value will favour less creative responses, which is exactly what we are looking for in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key learnings\n",
    "- Following a chat template is highly recommented in order to mimick model's training process (therefore, model's knowledge).\n",
    "- Prompt engineering is mandatory: a poor prompt will lead to poor results. Techniques such as Few-Shot learning can be useful tools in our arsenale.\n",
    "- Controlling the generation parameters is important and depends on the task, whether we seek creativity or factual responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stuffing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- import image from image path -->\n",
    "![image](image/image.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- import image from image path -->\n",
    "![image](image/mapreduce.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- import image from image path -->\n",
    "![image](image/refine.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document splitting strategies\n",
    "Tokens are pieces of words: when we write a prompt, the input is transformed into tokens. One token doens't mean one word, but we can generally approximate 1 token ~ 4 characters in English ~ 3/4 of a word. Simply put, 75 words ~ 100 tokens.\n",
    "\n",
    "Depending on the model used, we can accomodate a certain amount of tokens shared between prompt and model's generation, thus forcing us to operate some splitting if the context is too large. Gemma has a maximum context length of 8192 tokens, which roughly translates to more than 6100 words.\n",
    "\n",
    "Scenario C - Output is poor and writeups don't follow a clear structure: This is the most difficult, yet plausible, scenario, in which our model struggles with the winner's stream of consciousness and lack of a clear document structure. Moreover, if the writeup is lengthy, the situation could be particularly challenging. In such cases, a character splitting strategy could be ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see it in action, testing a formatted fake writeup from the [documentation](https://www.kaggle.com/solution-write-up-documentation) and a messy one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_clean_writeup = \"\"\"\n",
    "# Context section This # section only #contains 2 links # Data context link to the competition data page # Overview of the Approach this section should describe the models or algorithms used, describe the data preprocessing, feature engineering, and/or feature selection strategy, described the validation strategy.# Details of the submission this section should include what was special, creative, important, and/or impactful about the submission. And also, what was tried and didn’t work. # Sources this section should include links to helpful resources like research papers, past winning write-up solutions, forum posts, helpful notebooks, etc.\"\"\" \n",
    "\n",
    "example_messy_writeup = \"\"\"\n",
    "This section only contains 2 links, and here the link to the competition data page.\n",
    "Partial section.\n",
    "Another partial section.\n",
    "Extensive model secondi which describes the models or algorithms used, describe the data preprocessing, feature engineering, and/or feature selection strategy, described the validation strategy.\n",
    "Special section should include what was special, creative, important, and/or impactful about the submission. And also, what was tried and didn’t work. Last section should include links to helpful resources like research papers, past winning write-up solutions, forum posts, helpful notebooks, etc.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split clean writeup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 209, which is longer than the specified 100\n",
      "Created a chunk of size 175, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "# Context section This # section only #contains 2 links\n",
      "----------------\n",
      "section only #contains 2 links # Data context link to the competition data page\n",
      "----------------\n",
      "Overview of the Approach this section should describe the models or algorithms used, describe the data preprocessing, feature engineering, and/or feature selection strategy, described the validation strategy.\n",
      "----------------\n",
      "Details of the submission this section should include what was special, creative, important, and/or impactful about the submission. And also, what was tried and didn’t work.\n",
      "----------------\n",
      "Sources this section should include links to helpful resources like research papers, past winning write-up solutions, forum posts, helpful notebooks, etc.\n"
     ]
    }
   ],
   "source": [
    "# Split the clean writeup based on sections\n",
    "text_splitter = CharacterTextSplitter(separator='#', chunk_size=100, chunk_overlap=50)\n",
    "texts_clean_writeup = text_splitter.split_text(example_clean_writeup)\n",
    "\n",
    "# Print the first characters of each split\n",
    "for i in texts_clean_writeup:\n",
    "    print(\"----------------\")\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split  messy writeup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 194, which is longer than the specified 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "This section only contains 2 links, and here the link to the competition data page.\n",
      "Partial section.\n",
      "----------------\n",
      "Partial section.\n",
      "Another partial section.\n",
      "----------------\n",
      "Extensive model secondi which describes the models or algorithms used, describe the data preprocessing, feature engineering, and/or feature selection strategy, described the validation strategy.\n",
      "----------------\n",
      "Special section should include what was special, creative, important, and/or impactful about the submission. And also, what was tried and didn’t work. Last section should include links to helpful resources like research papers, past winning write-up solutions, forum posts, helpful notebooks, etc.\n"
     ]
    }
   ],
   "source": [
    "example_messy_writeup = \"\"\"\n",
    "This section only contains 2 links, and here the link to the competition data page.\n",
    "Partial section.\n",
    "Another partial section.\n",
    "Extensive model secondi which describes the models or algorithms used, describe the data preprocessing, feature engineering, and/or feature selection strategy, described the validation strategy.\n",
    "Special section should include what was special, creative, important, and/or impactful about the submission. And also, what was tried and didn’t work. Last section should include links to helpful resources like research papers, past winning write-up solutions, forum posts, helpful notebooks, etc.\"\"\"\n",
    "# Split the messy writeup based on newlines\n",
    "text_splitter = CharacterTextSplitter(separator='\\n', chunk_size=100, chunk_overlap=50)\n",
    "texts_messy_writeup = text_splitter.split_text(example_messy_writeup)\n",
    "\n",
    "for i in texts_messy_writeup:\n",
    "    print(\"----------------\")\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split by HTML header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h2>TLDR</h2>\n",
      "<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.</p>\n",
      "<p>We used only competition data.</p>\n",
      "<h2>1. Data Preprocessing</h2>\n",
      "<h3>1.1 CNN Preprocessing</h3>\n",
      "<ul>\n",
      "<li>We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points.</li>\n",
      "<li>During training, we applied various augmentations.</li>\n",
      "<li>We implemented standard normalization.</li>\n",
      "<li>Instead of dropping NaN values, we filled them with zeros after normalization.</li>\n",
      "<li>We interpolated the time axis to a size of 160 using 'nearest' interpolation: <code>yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest')</code>.</li>\n",
      "<li>Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the <code>(X, Y, Z)</code> axes. <img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2Fc47290891a7ac6497a6a0c296973f071%2Fdata_prep.jpg?generation=1682986293067532&amp;alt=media\" alt=\"Preprocessing\"></li>\n",
      "</ul>\n",
      "<h3>1.2 Transformer Preprocessing</h3>\n",
      "<ul>\n",
      "<li><p>Only 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.</p></li>\n",
      "<li><p>Augmentations, normalization and NaN-filling were applied sequentially.</p></li>\n",
      "<li><p>Sequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.</p></li>\n",
      "<li><p>Apart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.</p></li>\n",
      "<li><p>Motion features consist of future motion and history motion, which can be denoted as:</p></li>\n",
      "</ul>\n",
      "<p>$$<br>\n",
      "  Motion_{future} = position_{t+1} - position_{t}<br>\n",
      "$$<br>\n",
      "$$<br>\n",
      "  Motion_{history} = position_{t} - position_{t-1}<br>\n",
      "$$</p>\n",
      "<ul>\n",
      "<li><p>Full 210 pairwise distances among 21 hand points were included. </p></li>\n",
      "<li><p>There are 5 vertices in a finger (e.g. thumb is <code>[0,1,2,3,4]</code>), and therefore, there are 3 angles: <code>&lt;0,1,2&gt;, &lt;1,2,3&gt;, &lt;2,3,4&gt;</code>. So 15 angles of 5 fingers were included.</p></li>\n",
      "<li><p>Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.</p></li>\n",
      "</ul>\n",
      "<h2>2. Augmentation</h2>\n",
      "<h3>2.1 Common Augmentations</h3>\n",
      "<blockquote>\n",
      "  <p>These augmentations are used in both CNN training and transformer training</p>\n",
      "</blockquote>\n",
      "<ol>\n",
      "<li><p><code>Random affine</code>: Same as <a href=\"https://www.kaggle.com/hengck23\" target=\"_blank\">@hengck23</a> shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).</p></li>\n",
      "<li><p><code>Random interpolation</code>: Slightly scale and shift the time dimension.</p></li>\n",
      "<li><p><code>Flip pose</code>: Flip the x-coordinates of all points. In CNN, <code>x_new = x_max - x_old</code>. In transformer, <code>x_new = 2 * frame[:,0,0] - x_old</code>.</p></li>\n",
      "<li><p><code>Finger tree rotate</code>: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb (<code>[0,1,2,3,4]</code>), these 4 root-children pairs are: <code>0-[1,2,3,4]</code>,<code>1-[2,3,4]</code>,<code>2-[3,4]</code>,<code>3-[4]</code>. We randomly choose some of these pairs, and rotate the children points around root point with a small random angle.</p></li>\n",
      "</ol>\n",
      "<h3>2.2 CNN Specific Augmentations</h3>\n",
      "<ul>\n",
      "<li><code>Mixup</code>: Implement basic mixup augmentation (only works with CNNs, not transformers).</li>\n",
      "<li><code>Replace augmentation</code>: Replace some random parts from other samples of the same class.</li>\n",
      "<li><code>Time and frequence masking</code>: This basic torchaudio augmentation works exceptionally well.</li>\n",
      "</ul>\n",
      "<pre><code>freq_m = torchaudio.transforms.FrequencyMasking()  \n",
      "time_m = torchaudio.transforms.TimeMasking()       \n",
      "</code></pre>\n",
      "<h3>2.3 Augmented Sample Example</h3>\n",
      "<p>Before augmentation:</p>\n",
      "<p><img alt=\"aug1\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F5d2b97cf6754c5f4063724181bfe7172%2Fbefore_aug.png?generation=1682986332091937&amp;alt=media\"> </p>\n",
      "<p>After augmentation:</p>\n",
      "<p> <img alt=\"aug2\" src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4212496%2F516f0790f6f9903ad8943977bc392965%2Fafter_aug.png?generation=1682986413818912&amp;alt=media\"> </p>\n",
      "<h2>3. Training</h2>\n",
      "<h3>3.1 CNN Training</h3>\n",
      "<ul>\n",
      "<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\n",
      "<li>Onecycle scheduler with 0.1 warmup.</li>\n",
      "<li>Use weighted <code>CrossEntropyLoss</code>. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat)</li>\n",
      "<li>Implement a hypercolumn for EfficientNet with 5 blocks</li>\n",
      "</ul>\n",
      "<h3>3.2 Transformer Training</h3>\n",
      "<ul>\n",
      "<li>Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters</li>\n",
      "<li>Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule.</li>\n",
      "<li>A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained.</li>\n",
      "<li>A 3-layer model was initialized with 4-layer model's first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.</li>\n",
      "</ul>\n",
      "<h3>3.3 Hyperparameter Tuning</h3>\n",
      "<p>Since we trained only one fold and used smaller models, we decided to tune most parameters with Optuna. </p>\n",
      "<p>Here is the parameters list of CNN training (transformer training has a similar param-list):</p>\n",
      "<ul>\n",
      "<li><p>All augmentations probabilities (0.1 - 0.5+)</p></li>\n",
      "<li><p>Learning rate (2e-3 - 3e-3)</p></li>\n",
      "<li><p>Drop out (0.1 - 0.25)</p></li>\n",
      "<li><p>Num of epochs (170-185)</p></li>\n",
      "<li><p>Loss weights powers (0.75 - 2)</p></li>\n",
      "<li><p>Optimizer (<code>Lookahead_RAdam</code>, <code>RAdam</code>)</p></li>\n",
      "<li><p>Label smoothing (0.5 - 0.7)</p></li>\n",
      "</ul>\n",
      "<h2>4. Submissions, Conversion and Ensemble</h2>\n",
      "<ol>\n",
      "<li><p>We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%. For transformer model, pytorch-onnx-tf-tflite will generate too much useless tensor shape operations, a fully rewriting can reduce these manually. For CNN model, we rewrote DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.</p></li>\n",
      "<li><p>After that, we aggregated all these models in the <code>tf.Module</code> class. Converting directly from Keras resulted in lower speed (don't know why).</p></li>\n",
      "<li><p>We calculated ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.</p></li>\n",
      "</ol>\n",
      "<p>EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81. The final ensemble included:</p>\n",
      "<ol>\n",
      "<li>Efficientnet-B0, fold 0</li>\n",
      "<li>BERT, full data train</li>\n",
      "<li>DeBERTa, full data train</li>\n",
      "</ol>\n",
      "<p>Interestingly, a key feature was using the ensemble without softmax, which consistently provided a boost of around 0.01.</p>\n",
      "<h2>5. PS. Need <strong>BETTER</strong> TFlite DepthwiseConv2D</h2>\n",
      "<p>Depthwise convolution models performed very well for these tasks, outperforming other CNN and ViT models (rexnet_100 was also good).<br>\n",
      "We spent a lot of time dealing with the conversion of DepthwiseConv2D operation. Here are some strange results:</p>\n",
      "<p>Given a input image with 82x42x32 (HWC), there are two ways to do a 3x3 depthwise convolution in Keras. One is <code>Conv2D(32, 3, groups = 32)</code>, the other is <code>DepthwiseConv2D(3)</code>. However, after converting these two to tflite, the running time of the <code>Conv2D</code> is 5.05ms, and the running time of <code>DepthwiseConv2D</code> is 3.70ms. More strangely, a full convolution <code>Conv2D(32, 3, groups = 1)</code> with FLOPs = HWC^2 only takes 2.09ms, even faster than previous two with FLOPs = HWC.</p>\n",
      "<p>Then we rewrote the depthwise-conv like this:</p>\n",
      "<pre><code>     ():\n",
      "        out = x[:,:self.H_out:self.strides,:self.W_out:self.strides] * self.weight[,]\n",
      "         i  (self.kernel_size):\n",
      "             j  (self.kernel_size):\n",
      "                 i ==   j == :\n",
      "                    \n",
      "                out += x[:,i:self.H_out + i:self.strides,j:self.W_out + j:self.strides] * self.weight[i,j]\n",
      "         self.bias   :\n",
      "            out = out + self.bias\n",
      "         out\n",
      "</code></pre>\n",
      "<p>The running time of this is 1.24 ms.</p>\n",
      "<p>In summary, our version (1.24ms) &gt; full <code>Conv2D</code> with larger FLOPs (2.09ms) &gt; <code>DepthwiseConv2D</code> (3.70ms) &gt; <code>Conv2D(C, groups = C)</code> (5.05ms).</p>\n",
      "<p>However, our version introduced too much nodes in tflite graph, which is not stable in running time. If the tensorflow team has a better implementation of DepthwiseConv2D, we can even ensemble two CNN models, which is expected to reach 0.82 LB.</p>\n",
      "<p>By the way, EfficientNet with ONNX was ~5 times faster than TFLite.</p>\n",
      "<h3>Big thanks to my teammates <a href=\"https://www.kaggle.com/artemtprv\" target=\"_blank\">@artemtprv</a> and <a href=\"https://www.kaggle.com/carnozhao\" target=\"_blank\">@carnozhao</a> and congrats with new tiers, Master and GrandMaster!</h3>\n",
      "<p><a href=\"https://github.com/ffs333/2nd_place_GISLR\" target=\"_blank\">github code</a></p>\n"
     ]
    }
   ],
   "source": [
    "print(writeup_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `return_each_element` in this function will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length writup: 9864\n",
      "Number of splits: 6\n",
      "Element returned: <class 'langchain_core.documents.base.Document'>\n",
      "Length of each split: [511, 1567, 1040, 1203, 1031, 1260]\n",
      "\n",
      "[('We used an approach similar to audio spectrogram c', {'Header 2': 'TLDR'}), ('We extracted 18 lip points, 20 pose points (includ', {'Header 2': '1. Data Preprocessing'}), ('These augmentations are used in both CNN training ', {'Header 2': '2. Augmentation'}), ('Train on one fold with a random split (8 folds in ', {'Header 2': '3. Training'}), ('We rewrote all our models in Keras and transferred', {'Header 2': '4. Submissions, Conversion and Ensemble'}), ('Depthwise convolution models performed very well f', {'Header 2': '5. PS. Need BETTER TFlite DepthwiseConv2D'})]\n"
     ]
    }
   ],
   "source": [
    "# Split on HTML headers\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\")\n",
    "]\n",
    "\n",
    "# Split the real HTML writeup based on headers\n",
    "text_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on, \n",
    "                                       return_each_element=False\n",
    "                                       )\n",
    "\n",
    "texts_html_writeup = text_splitter.split_text(writeup_small)\n",
    "\n",
    "print('Length writup:', len(writeup_small))\n",
    "print('Number of splits:', len(texts_html_writeup))\n",
    "print('Element returned:', type(texts_html_writeup[0]))\n",
    "print('Length of each split:', [len(i.page_content) for i in texts_html_writeup])\n",
    "\n",
    "# Print the first characters for each split\n",
    "print(); print([(i.page_content[:50], i.metadata) for i in texts_html_writeup])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TLDR\n",
      "We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.  \n",
      "We used only competition data.\n",
      "\n",
      "1. Data Preprocessing\n",
      "We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points. During training, we applied various augmentations. We implemented standard normalization. Instead of dropping NaN values, we filled them with zeros after normalization. We interpolated the time axis to a size of 160 using 'nearest' interpolation: yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest'). Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the (X, Y, Z) axes.  \n",
      "Only 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.  \n",
      "Augmentations, normalization and NaN-filling were applied sequentially.  \n",
      "Sequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.  \n",
      "Apart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.  \n",
      "Motion features consist of future motion and history motion, which can be denoted as:  \n",
      "$$ Motion_{future} = position_{t+1} - position_{t} $$ $$ Motion_{history} = position_{t} - position_{t-1} $$  \n",
      "Full 210 pairwise distances among 21 hand points were included.  \n",
      "There are 5 vertices in a finger (e.g. thumb is [0,1,2,3,4]), and therefore, there are 3 angles: <0,1,2>, <1,2,3>, <2,3,4>. So 15 angles of 5 fingers were included.  \n",
      "Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, text in enumerate(texts_html_writeup):\n",
    "    # Join the metadata and the content together\n",
    "    final_content = '\\n'.join(text.metadata.values()) + '\\n' + text.page_content\n",
    "    # Replace the old content with the enriched one\n",
    "    text.page_content = final_content\n",
    "    \n",
    "    # Print some examples\n",
    "    if i < 2:\n",
    "        print(final_content); print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CharacterTextSplitter again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of final splits: 6\n",
      "Length of each final split: [516, 1589, 1056, 1215, 1071, 1302]\n",
      "\n",
      "[('TLDR\\nWe used an approach similar to audio spectrog', {'Header 2': 'TLDR'}), ('1. Data Preprocessing\\nWe extracted 18 lip points, ', {'Header 2': '1. Data Preprocessing'}), ('2. Augmentation\\nThese augmentations are used in bo', {'Header 2': '2. Augmentation'}), ('3. Training\\nTrain on one fold with a random split ', {'Header 2': '3. Training'}), ('4. Submissions, Conversion and Ensemble\\nWe rewrote', {'Header 2': '4. Submissions, Conversion and Ensemble'}), ('5. PS. Need BETTER TFlite DepthwiseConv2D\\nDepthwis', {'Header 2': '5. PS. Need BETTER TFlite DepthwiseConv2D'})]\n"
     ]
    }
   ],
   "source": [
    "text_splitter = langchain.text_splitter.CharacterTextSplitter(chunk_size=2000, chunk_overlap=100)\n",
    "\n",
    "# Split\n",
    "splits = text_splitter.split_documents(texts_html_writeup)\n",
    "print('Number of final splits:', len(splits))\n",
    "print('Length of each final split:', [len(i.page_content) for i in splits])\n",
    "\n",
    "print(); print([(i.page_content[:50], i.metadata) for i in splits])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key learnings\n",
    "\n",
    "- Stuffing, MapReduce and Refine are three different techniques that can be used to summarize documents.\n",
    "- Given the task, document structure and model capabilities, we might need to split our document in chunks to fit the context in our prompt or to improve the summary.\n",
    "- Kaggle writeups can all potentially fit in Gemma given its context length, but different strategies such as Sections splitting based on HTML formatting could potentially be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "class GemmaLLM(LLM):\n",
    "    hf_pipe: Any = None\n",
    "    pipe_kwargs: Any = None\n",
    "        \n",
    "    def __init__(self, hf_pipeline, pipe_kwargs):\n",
    "        super(GemmaLLM, self).__init__()\n",
    "        self.hf_pipe = hf_pipeline\n",
    "        self.pipe_kwargs = pipe_kwargs\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"Gemma pipeline\"\n",
    "\n",
    "    def _call(self, prompt, **kwargs):\n",
    "        \"\"\"\n",
    "        This is the part that gets invoked by LangChain. We make sure that we pass the parameters we\n",
    "        previously discussed to the HF pipeline, returning only the output without the prompt.\n",
    "        \"\"\"\n",
    "        outputs = self.hf_pipe(\n",
    "            prompt,\n",
    "            do_sample=self.pipe_kwargs['do_sample'],\n",
    "            temperature=self.pipe_kwargs['temperature'],\n",
    "            top_k=self.pipe_kwargs['top_k'],\n",
    "            top_p=self.pipe_kwargs['top_p'],\n",
    "            add_special_tokens=self.pipe_kwargs['add_special_tokens']\n",
    "        )\n",
    "        return outputs[0][\"generated_text\"][len(prompt):]  \n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        \"\"\"Pipeline params\"\"\"\n",
    "        return {\"n\": self.pipe_kwargs}\n",
    "\n",
    "langchain_hf = GemmaLLM(hf_pipeline=pipe,\n",
    "                        pipe_kwargs={\n",
    "                            'do_sample':True,\n",
    "                            'temperature':0.1,\n",
    "                            'top_k':20,\n",
    "                            'top_p':0.3,\n",
    "                            'add_special_tokens':True\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start_of_turn>user\\nSummarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\\n\\n<h2>TLDR</h2>\\n<p>We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models s'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt[:350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's a summary of the text in a technical way:\n",
      "\n",
      "**1. Data Preprocessing**\n",
      "\n",
      "* Extract 80 points from the image, including lip and pose points.\n",
      "* Apply various augmentations and normalizations.\n",
      "* Fill NaN values with zeros and use nearest interpolation for the time axis.\n",
      "\n",
      "**2. Augmentation**\n",
      "\n",
      "* Use common and CNN specific augmentations.\n",
      "* Implement a mixup augmentation that only works with CNNs.\n",
      "\n",
      "**3. Training**\n",
      "\n",
      "* Train EfficientNet-B0 and BERT on a single fold with 0.1 warm-up.\n",
      "* Train a transformer model with a ranger optimizer and 4-layer transformer.\n",
      "* Tune hyperparameters with Optuna.\n",
      "\n",
      "**4. Submissions**\n",
      "\n",
      "* Aggregate models in a tf.Module.\n",
      "* Calculate ensemble weights for fold 0 and apply to the full dataset.\n",
      "\n",
      "**5. PS. Need BETTER TFlite DepthwiseConv2D**\n",
      "\n",
      "* Explore different ways to implement depthwise convolution in tflite.\n",
      "* Experiment with different FLOP configurations.\n",
      "\n",
      "**6. Conclusion**\n",
      "\n",
      "* EfficientNet-B0 achieved a leaderboard score of 0.8.\n",
      "* Transformers improved the score to 0.81.\n",
      "* Ensemble of models with different architectures achieved the highest score of 0.82.\n"
     ]
    }
   ],
   "source": [
    "out = langchain_hf.invoke(prompt)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    CompletionResponseGen,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "\n",
    "class Gemma(CustomLLM):\n",
    "    num_output: int = 512\n",
    "    model_name: str = \"Gemma\"\n",
    "    model: Any = None\n",
    "\n",
    "    def __init__(self, model, num_output):\n",
    "        super(Gemma, self).__init__()\n",
    "        self.model = model\n",
    "        self.num_output = num_output\n",
    "\n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"Get LLM metadata.\"\"\"\n",
    "        return LLMMetadata(\n",
    "            num_output=self.num_output,\n",
    "            model_name=self.model_name,\n",
    "        )\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        return CompletionResponse(text=self.model.generate(prompt, max_length=self.num_output))\n",
    "\n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any) -> CompletionResponseGen:\n",
    "        response = \"\"\n",
    "        for token in self.model.generate(prompt, max_length=self.num_output):\n",
    "            response += token\n",
    "            yield CompletionResponse(text=response, delta=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gemma_lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response \u001b[38;5;241m=\u001b[39m Gemma(\u001b[43mgemma_lm\u001b[49m, \u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mcomplete(sample_query)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gemma_lm' is not defined"
     ]
    }
   ],
   "source": [
    "# response = Gemma(gemma_lm, 512).complete(sample_query)\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1: EfficientNet-B0\n",
      "* The EfficientNet-B0 model is a deep neural network architecture that is designed to be efficient.\n",
      "* The model consists of a hierarchy of depthwise convolutions, followed by a global average pooling layer.\n",
      "* The model is trained using a single fold of 8 randomly split folds.\n",
      "**Chapter 1: Data Preparation**\n",
      "* The dataset consists of 10,000 images with 10 classes.\n",
      "* The EfficientNet-B0 model is trained on a single fold with the following settings:\n",
      "    * Input size: 160x80\n",
      "    * Number of filters: 512\n",
      "    * Number of layers: 19\n",
      "    * Batch size: 32\n",
      "    * Learning rate: 0.001\n",
      "**Chapter 2: Model Training**\n",
      "* The EfficientNet-B0 model is trained on the single fold with the following settings:\n",
      "    * Input size: 160x80\n",
      "    * Number of filters: 512\n",
      "    * Number of layers: 19\n",
      "    * Batch size: 32\n",
      "    * Learning rate: 0.001\n",
      "**Chapter 3: Evaluation**\n",
      "* The model is evaluated on the single fold with the following metrics:\n",
      "    * CV score: 0.898\n",
      "    * Leaderboard score: ~0.8\n",
      "**Chapter 4: Results and Discussion**\n",
      "* The EfficientNet-B0 model with the single fold training achieves a CV score of 0.898.\n",
      "* The EfficientNet-B0 model with the single fold training achieves a leaderboard score of ~0.8.\n",
      "* The model is trained on only competition data.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define prompt for summarization of each chunk\n",
    "prompt_template = \"\"\"<bos><start_of_turn>user\n",
    "Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n",
    "\n",
    "{text}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "prompt_init = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define prompt for final output, the summary of summaries\n",
    "combine_template = \"\"\"<bos><start_of_turn>user\n",
    "You are given a text containing summaries of different part of a document.\n",
    "Create one single summary combining all the information of the chapters. Divide the summary in chapters, be impersonal and use bullet points:\n",
    "\n",
    "{text}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "combine_prompt = PromptTemplate.from_template(combine_template)\n",
    "\n",
    "# Create the chain of summarization, using map_reduce\n",
    "chain = load_summarize_chain(langchain_hf, chain_type='map_reduce', map_prompt=prompt_init, combine_prompt=combine_prompt)\n",
    "\n",
    "# Run the chain on the chunks\n",
    "out_summary = chain.invoke(splits)\n",
    "print(out_summary['output_text'].replace('\\n\\n','\\n'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n",
      "\n",
      "TLDR\n",
      "We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.  \n",
      "We used only competition data.<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n",
      "\n",
      "1. Data Preprocessing\n",
      "We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points. During training, we applied various augmentations. We implemented standard normalization. Instead of dropping NaN values, we filled them with zeros after normalization. We interpolated the time axis to a size of 160 using 'nearest' interpolation: yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest'). Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the (X, Y, Z) axes.  \n",
      "Only 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.  \n",
      "Augmentations, normalization and NaN-filling were applied sequentially.  \n",
      "Sequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.  \n",
      "Apart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.  \n",
      "Motion features consist of future motion and history motion, which can be denoted as:  \n",
      "$$ Motion_{future} = position_{t+1} - position_{t} $$ $$ Motion_{history} = position_{t} - position_{t-1} $$  \n",
      "Full 210 pairwise distances among 21 hand points were included.  \n",
      "There are 5 vertices in a finger (e.g. thumb is [0,1,2,3,4]), and therefore, there are 3 angles: <0,1,2>, <1,2,3>, <2,3,4>. So 15 angles of 5 fingers were included.  \n",
      "Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n",
      "\n",
      "2. Augmentation\n",
      "These augmentations are used in both CNN training and transformer training  \n",
      "Random affine: Same as @hengck23 shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).  \n",
      "Random interpolation: Slightly scale and shift the time dimension.  \n",
      "Flip pose: Flip the x-coordinates of all points. In CNN, x_new = x_max - x_old. In transformer, x_new = 2 * frame[:,0,0] - x_old.  \n",
      "Finger tree rotate: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb ([0,1,2,3,4]), these 4 root-children pairs are: 0-[1,2,3,4],1-[2,3,4],2-[3,4],3-[4]. We randomly choose some of these pairs, and rotate the children points around root point with a small random angle.  \n",
      "Mixup: Implement basic mixup augmentation (only works with CNNs, not transformers). Replace augmentation: Replace some random parts from other samples of the same class. Time and frequence masking: This basic torchaudio augmentation works exceptionally well.  \n",
      "Before augmentation:  \n",
      "After augmentation:<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n",
      "\n",
      "3. Training\n",
      "Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters Onecycle scheduler with 0.1 warmup. Use weighted CrossEntropyLoss. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat) Implement a hypercolumn for EfficientNet with 5 blocks  \n",
      "Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule. A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained. A 3-layer model was initialized with 4-layer model's first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.  \n",
      "Since we trained only one fold and used smaller models, we decided to tune most parameters with Optuna.  \n",
      "Here is the parameters list of CNN training (transformer training has a similar param-list):  \n",
      "All augmentations probabilities (0.1 - 0.5+)  \n",
      "Learning rate (2e-3 - 3e-3)  \n",
      "Drop out (0.1 - 0.25)  \n",
      "Num of epochs (170-185)  \n",
      "Loss weights powers (0.75 - 2)  \n",
      "Optimizer (Lookahead_RAdam, RAdam)  \n",
      "Label smoothing (0.5 - 0.7)<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n",
      "\n",
      "4. Submissions, Conversion and Ensemble\n",
      "We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%. For transformer model, pytorch-onnx-tf-tflite will generate too much useless tensor shape operations, a fully rewriting can reduce these manually. For CNN model, we rewrote DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.  \n",
      "After that, we aggregated all these models in the tf.Module class. Converting directly from Keras resulted in lower speed (don't know why).  \n",
      "We calculated ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.  \n",
      "EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81. The final ensemble included:  \n",
      "Efficientnet-B0, fold 0 BERT, full data train DeBERTa, full data train  \n",
      "Interestingly, a key feature was using the ensemble without softmax, which consistently provided a boost of around 0.01.<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n",
      "\n",
      "5. PS. Need BETTER TFlite DepthwiseConv2D\n",
      "Depthwise convolution models performed very well for these tasks, outperforming other CNN and ViT models (rexnet_100 was also good). We spent a lot of time dealing with the conversion of DepthwiseConv2D operation. Here are some strange results:  \n",
      "Given a input image with 82x42x32 (HWC), there are two ways to do a 3x3 depthwise convolution in Keras. One is Conv2D(32, 3, groups = 32), the other is DepthwiseConv2D(3). However, after converting these two to tflite, the running time of the Conv2D is 5.05ms, and the running time of DepthwiseConv2D is 3.70ms. More strangely, a full convolution Conv2D(32, 3, groups = 1) with FLOPs = HWC^2 only takes 2.09ms, even faster than previous two with FLOPs = HWC.  \n",
      "Then we rewrote the depthwise-conv like this:  \n",
      "The running time of this is 1.24 ms.  \n",
      "In summary, our version (1.24ms) > full Conv2D with larger FLOPs (2.09ms) > DepthwiseConv2D (3.70ms) > Conv2D(C, groups = C) (5.05ms).  \n",
      "However, our version introduced too much nodes in tflite graph, which is not stable in running time. If the tensorflow team has a better implementation of DepthwiseConv2D, we can even ensemble two CNN models, which is expected to reach 0.82 LB.  \n",
      "By the way, EfficientNet with ONNX was ~5 times faster than TFLite.  \n",
      "github code<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "You are given a text containing summaries of different part of a document.\n",
      "Create one single summary combining all the information of the chapters. Divide the summary in chapters, be impersonal and use bullet points:\n",
      "\n",
      " architecture:\n",
      "\n",
      "* EfficientNet-B0 model\n",
      "* Transformer models (BERT and DeBERTa) as helper models\n",
      "* Single fold training with 8 splits\n",
      "* Use of competition data only\n",
      "</start_of_turn>\n",
      "\n",
      "**Chapter 1: Data Preparation**\n",
      "\n",
      "* The dataset consists of 10,000 images with 10 classes.\n",
      "* The EfficientNet-B0 model is used as the base model.\n",
      "* The BERT and DeBERTa models are used as helper models.\n",
      "* The model is trained on a single fold from 8 randomly split folds.\n",
      "\n",
      "**Chapter 2: Model Training**\n",
      "\n",
      "* The EfficientNet-B0 model is trained on the single fold with the following settings:\n",
      "    * Input size: 160x80\n",
      "    * Number of filters: 512\n",
      "    * Number of layers: 19\n",
      "    * Batch size: 32\n",
      "    * Learning rate: 0.001\n",
      "* The BERT and DeBERTa models are trained on the full dataset with the following settings:\n",
      "    * Input size: 128\n",
      "    * Number of filters: 512\n",
      "    * Number of layers: 12\n",
      "    * Batch size: 16\n",
      "    * Learning rate: 0.001\n",
      "\n",
      "**Chapter 3: Evaluation**\n",
      "\n",
      "* The model is evaluated on the single fold with the following metrics:\n",
      "    * CV score: 0.898\n",
      "    * Leaderboard score: ~0.8\n",
      "\n",
      "**Chapter 4: Results and Discussion**\n",
      "\n",
      "* The EfficientNet-B0 model with the single fold training achieves a CV score of 0.898.\n",
      "* The BERT and DeBERTa models achieve a leaderboard score of ~0.8.\n",
      "* The model is trained on only competition data.\n",
      "\n",
      " 1: Data Preprocessing\n",
      "\n",
      "- Extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points.\n",
      "- Applied various augmentations.\n",
      "- Implemented standard normalization.\n",
      "- Filled NaN values with zeros after normalization.\n",
      "- Interpolated the time axis to a size of 160 using 'nearest' interpolation.\n",
      "- Obtained a tensor with dimensions 160x80x3.\n",
      "\n",
      "<start_of_turn>model 2: Feature Extraction\n",
      "\n",
      "- Hand-crafted features were also used, including motion, distances, and cosine of angles.\n",
      "\n",
      "<start_of_turn>model 3: Feature Engineering\n",
      "\n",
      "- Motion features consist of future motion and history motion.\n",
      "- Full 210 pairwise distances among 21 hand points were included.\n",
      "- 15 angles of 5 fingers were included.\n",
      "- Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.\n",
      "\n",
      ":\n",
      "**Chapter 1: Augmentation Techniques**\n",
      "\n",
      "* **Random affine:** Randomly shifts and scales each part of the image.\n",
      "* **Random interpolation:** Randomly scales and shifts the time dimension of the image.\n",
      "* **Flip pose:** Flip the x-coordinates of all points in the image.\n",
      "* **Finger tree rotate:** Randomly rotates the children points of the finger tree.\n",
      "* **Mixup:** Replace some random parts from other samples of the same class.\n",
      "\n",
      "**Chapter 2: Time and Frequency Masking**\n",
      "\n",
      "* This technique is used to increase the diversity of the training data.\n",
      "* It involves randomly masking out parts of the image and then replacing them with a different part of the image.\n",
      "* This technique is effective in reducing overfitting and improving thegeneralizability of the model.\n",
      "\n",
      " architecture:\n",
      "\n",
      "- Input size: 224x224x3\n",
      "- Output size: 768\n",
      "- Number of filters: 512\n",
      "- Number of hidden units: 256\n",
      "- Number of layers: 4\n",
      "- Activation function: ReLU\n",
      "- Loss function: Weighted CrossEntropyLoss\n",
      "- Optimizer: Lookahead_RAdam\n",
      "- Learning rate: 2e-3 - 3e-3\n",
      "- Dropout: 0.1 - 0.25\n",
      "- Number of epochs: 170-185\n",
      "- Loss weight power: 0.75 - 2\n",
      "- Number of classes: 10\n",
      "\n",
      "**Chapter 1: Training**\n",
      "\n",
      "- Use a random split of the data to divide the dataset into 8 folds.\n",
      "- Train the model on each fold with the best parameters found by Optuna.\n",
      "- Use weighted CrossEntropyLoss to penalize misclassification.\n",
      "- Increase the weights for poorly predicted classes and classes with semantically similar pairs.\n",
      "\n",
      "**Chapter 2: Model Architecture**\n",
      "\n",
      "- Use a 4-layer, 256 hidden-size, 512 intermediate-size transformer.\n",
      "- Initialize the 3-layer model with the first 3 layers of the 4-layer model.\n",
      "- Use knowledge distillation to transfer knowledge from the 4-layer model to the 3-layer model.\n",
      "\n",
      "**Chapter 3: Optimization**\n",
      "\n",
      "- Use Optuna to tune most parameters of the model.\n",
      "- Set the learning rate, drop out, epochs, and loss weight power as hyperparameters.\n",
      "- Use the Lookahead_RAdam optimizer with a learning rate schedule.\n",
      "- Use the Weighted CrossEntropyLoss function.\n",
      "\n",
      "_summary\n",
      "\n",
      "**Chapter 1: Model Rewriting**\n",
      "\n",
      "* Rewrote all models in Keras and transferred PyTorch weights to them.\n",
      "* For transformer model, pytorch-onnx-tf-tflite generated too much useless tensor shape operations.\n",
      "* For CNN model, DepthwiseConv2D was rewritten with a hard-coded way, whose speed was 200%~300% of its original version of tflite DepthwiseConv2D.\n",
      "\n",
      "**Chapter 2: Ensemble**\n",
      "\n",
      "* Calculated ensemble weights for models trained on fold 0 using the local fold 0 score.\n",
      "* Applied these weights to the full dataset models.\n",
      "\n",
      "**Chapter 3: Results**\n",
      "\n",
      "* EfficientNet-B0 achieved a leaderboard score of approximately 0.8.\n",
      "* Transformers improved the score to 0.81.\n",
      "* The final ensemble included Efficientnet-B0, fold 0 BERT, full data train DeBERTa, and full data train.\n",
      "\n",
      " summary:\n",
      "\n",
      "**Chapter 1: Introduction**\n",
      "\n",
      "* Depthwise convolution is a type of convolution that is used to extract features from images.\n",
      "* Traditional CNN and ViT models are both used for depthwise convolution, but they can be computationally expensive.\n",
      "* The goal of this paper is to compare the performance of different depthwise convolution implementations.\n",
      "\n",
      "**Chapter 2: Results**\n",
      "\n",
      "* The Conv2D operation was found to be the most efficient depthwise convolution implementation.\n",
      "* DepthwiseConv2D with 1 group was the fastest of the three methods tested, with a running time of 1.24 ms.\n",
      "* The full Conv2D operation with 32 groups took 2.09 ms, while the DepthwiseConv2D operation with 32 groups took 3.70 ms.\n",
      "* The EfficientNet model with ONNX was ~5 times faster than TFLite.\n",
      "\n",
      "**Chapter 3: Discussion**\n",
      "\n",
      "* The results show that DepthwiseConv2D with 1 group is the most efficient depthwise convolution implementation.\n",
      "* This is likely due to the fact that DepthwiseConv2D with 1 group is able to use a smaller kernel size, which can reduce the number of parameters that need to be learned.\n",
      "* The results also show that the running time of DepthwiseConv2D can be reduced by using a smaller kernel size.\n",
      "* The results suggest that the tensorflow team should consider improving the performance of DepthwiseConv2D.<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Repeat the process above, with verbose True\n",
    "chain = load_summarize_chain(langchain_hf, chain_type='map_reduce', verbose=True, map_prompt=prompt_init, combine_prompt=combine_prompt)\n",
    "\n",
    "# Run the chain on the chunks\n",
    "out_summary = chain.invoke(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n",
      "\n",
      "TLDR\n",
      "We used an approach similar to audio spectrogram classification using the EfficientNet-B0 model, with numerous augmentations and transformer models such as BERT and DeBERTa as helper models. The final solution consists of one EfficientNet-B0 with an input size of 160x80, trained on a single fold from 8 randomly split folds, as well as DeBERTa and BERT trained on the full dataset. A single fold model using EfficientNet has a CV score of 0.898 and a leaderboard score of ~0.8.  \n",
      "We used only competition data.<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Your job is to produce a final document divided in chapters and bullet points.\n",
      "You are given a text containing an existing summary to a certain point:\n",
      "\n",
      " architecture:\n",
      "\n",
      "* EfficientNet-B0 model\n",
      "* Transformer models (BERT and DeBERTa) as helper models\n",
      "* Single fold training with 8 splits\n",
      "* Use of competition data only\n",
      "</start_of_turn>\n",
      "\n",
      "**Chapter 1: Data Preparation**\n",
      "\n",
      "* The dataset consists of 10,000 images with 10 classes.\n",
      "* The EfficientNet-B0 model is used as the base model.\n",
      "* The BERT and DeBERTa models are used as helper models.\n",
      "* The model is trained on a single fold from 8 randomly split folds.\n",
      "\n",
      "**Chapter 2: Model Training**\n",
      "\n",
      "* The EfficientNet-B0 model is trained on the single fold with the following settings:\n",
      "    * Input size: 160x80\n",
      "    * Number of filters: 512\n",
      "    * Number of layers: 19\n",
      "    * Batch size: 32\n",
      "    * Learning rate: 0.001\n",
      "* The BERT and DeBERTa models are trained on the full dataset with the following settings:\n",
      "    * Input size: 128\n",
      "    * Number of filters: 512\n",
      "    * Number of layers: 12\n",
      "    * Batch size: 16\n",
      "    * Learning rate: 0.001\n",
      "\n",
      "**Chapter 3: Evaluation**\n",
      "\n",
      "* The model is evaluated on the single fold with the following metrics:\n",
      "    * CV score: 0.898\n",
      "    * Leaderboard score: ~0.8\n",
      "\n",
      "**Chapter 4: Results and Discussion**\n",
      "\n",
      "* The EfficientNet-B0 model with the single fold training achieves a CV score of 0.898.\n",
      "* The BERT and DeBERTa models achieve a leaderboard score of ~0.8.\n",
      "* The model is trained on only competition data.\n",
      "\n",
      "You can now refine it (if necessary) with more context below.\n",
      "\n",
      "1. Data Preprocessing\n",
      "We extracted 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points, resulting in a total of 80 points. During training, we applied various augmentations. We implemented standard normalization. Instead of dropping NaN values, we filled them with zeros after normalization. We interpolated the time axis to a size of 160 using 'nearest' interpolation: yy = F.interpolate(yy[None, None, :], size=self.new_size, mode='nearest'). Finally, we obtained a tensor with dimensions 160x80x3, where 3 represents the (X, Y, Z) axes.  \n",
      "Only 61 points were kept, including 40 lip points and 21 hand points. For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.  \n",
      "Augmentations, normalization and NaN-filling were applied sequentially.  \n",
      "Sequences longer than 96 were interpolated to 96. Sequences shorter than 96 were unchanged.  \n",
      "Apart from raw positions, hand-crafted features were also used, including motion, distances, and cosine of angles.  \n",
      "Motion features consist of future motion and history motion, which can be denoted as:  \n",
      "$$ Motion_{future} = position_{t+1} - position_{t} $$ $$ Motion_{history} = position_{t} - position_{t-1} $$  \n",
      "Full 210 pairwise distances among 21 hand points were included.  \n",
      "There are 5 vertices in a finger (e.g. thumb is [0,1,2,3,4]), and therefore, there are 3 angles: <0,1,2>, <1,2,3>, <2,3,4>. So 15 angles of 5 fingers were included.  \n",
      "Randomly selected 190 pairwise distances and randomly selected 8 angles among 40 lip points were included.\n",
      "\n",
      "Given the new context, refine the original summary.<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Your job is to produce a final document divided in chapters and bullet points.\n",
      "You are given a text containing an existing summary to a certain point:\n",
      "\n",
      " training\n",
      "\n",
      "The text describes the training of an architecture using the EfficientNet-B0 model and two helper models, BERT and DeBERTa. The model is trained on a single fold from 8 randomly split folds.\n",
      "\n",
      "**Data Preprocessing**\n",
      "\n",
      "* 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points were extracted from the input image.\n",
      "* Standard normalization was applied to all data.\n",
      "* The time axis was interpolated to a size of 160 using 'nearest' interpolation.\n",
      "* Only 61 points were kept, including 40 lip points and 21 hand points.\n",
      "* For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.\n",
      "\n",
      "**Model Training**\n",
      "\n",
      "* The EfficientNet-B0 model was trained on the single fold with the following settings:\n",
      "    * Input size: 160x80\n",
      "    * Number of filters: 512\n",
      "    * Number of layers: 19\n",
      "    * Batch size: 32\n",
      "    * Learning rate: 0.001\n",
      "* The BERT and DeBERTa models were trained on the full dataset with the following settings:\n",
      "    * Input size: 128\n",
      "    * Number of filters: 512\n",
      "    * Number of layers: 12\n",
      "    * Batch size: 16\n",
      "    * Learning rate: 0.001\n",
      "\n",
      "You can now refine it (if necessary) with more context below.\n",
      "\n",
      "2. Augmentation\n",
      "These augmentations are used in both CNN training and transformer training  \n",
      "Random affine: Same as @hengck23 shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).  \n",
      "Random interpolation: Slightly scale and shift the time dimension.  \n",
      "Flip pose: Flip the x-coordinates of all points. In CNN, x_new = x_max - x_old. In transformer, x_new = 2 * frame[:,0,0] - x_old.  \n",
      "Finger tree rotate: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb ([0,1,2,3,4]), these 4 root-children pairs are: 0-[1,2,3,4],1-[2,3,4],2-[3,4],3-[4]. We randomly choose some of these pairs, and rotate the children points around root point with a small random angle.  \n",
      "Mixup: Implement basic mixup augmentation (only works with CNNs, not transformers). Replace augmentation: Replace some random parts from other samples of the same class. Time and frequence masking: This basic torchaudio augmentation works exceptionally well.  \n",
      "Before augmentation:  \n",
      "After augmentation:\n",
      "\n",
      "Given the new context, refine the original summary.<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Your job is to produce a final document divided in chapters and bullet points.\n",
      "You are given a text containing an existing summary to a certain point:\n",
      "\n",
      " training\n",
      "\n",
      "The text describes the training of an architecture using the EfficientNet-B0 model and two helper models, BERT and DeBERTa. The model is trained on a single fold from 8 randomly split folds.\n",
      "\n",
      "**Data Preprocessing**\n",
      "\n",
      "* 18 lip points, 20 pose points (including arms, shoulders, eyebrows, and nose), and all hand points were extracted from the input image.\n",
      "* Standard normalization was applied to all data.\n",
      "* The time axis was interpolated to a size of 160 using 'nearest' interpolation.\n",
      "* Only 61 points were kept, including 40 lip points and 21 hand points.\n",
      "* For left and right hand, the one with less NaN was kept. If right hand was kept, mirror it to left hand.\n",
      "\n",
      "**Model Training**\n",
      "\n",
      "* The EfficientNet-B0 model was trained on the single fold with the following settings:\n",
      "    * Input size: 160x80\n",
      "    * Number of filters: 512\n",
      "    * Number of layers: 19\n",
      "    * Batch size: 32\n",
      "    * Learning rate: 0.001\n",
      "* The BERT and DeBERTa models were trained on the full dataset with the following settings:\n",
      "    * Input size: 128\n",
      "    * Number of filters: 512\n",
      "    * Number of layers: 12\n",
      "    * Batch size: 16\n",
      "    * Learning rate: 0.001\n",
      "\n",
      "**Additional Details**\n",
      "\n",
      "* Random affine: Same as @hengck23 shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).  \n",
      "* Random interpolation: Slightly scale and shift the time dimension.  \n",
      "* Flip pose: Flip the x-coordinates of all points. In CNN, x_new = x_max - x_old. In transformer, x_new = 2 * frame[:,0,0] - x_old.  \n",
      "* Finger tree rotate: There are 4 root-children pairs in a finger with 5-vertices. E.g. in thumb ([0,1,2,3,4]), these 4 root-children pairs are: 0-[1,2,3,4],1-[2,3,4],2-[3,4],3-[\n",
      "\n",
      "You can now refine it (if necessary) with more context below.\n",
      "\n",
      "3. Training\n",
      "Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters Onecycle scheduler with 0.1 warmup. Use weighted CrossEntropyLoss. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat) Implement a hypercolumn for EfficientNet with 5 blocks  \n",
      "Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule. A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained. A 3-layer model was initialized with 4-layer model's first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.  \n",
      "Since we trained only one fold and used smaller models, we decided to tune most parameters with Optuna.  \n",
      "Here is the parameters list of CNN training (transformer training has a similar param-list):  \n",
      "All augmentations probabilities (0.1 - 0.5+)  \n",
      "Learning rate (2e-3 - 3e-3)  \n",
      "Drop out (0.1 - 0.25)  \n",
      "Num of epochs (170-185)  \n",
      "Loss weights powers (0.75 - 2)  \n",
      "Optimizer (Lookahead_RAdam, RAdam)  \n",
      "Label smoothing (0.5 - 0.7)\n",
      "\n",
      "Given the new context, refine the original summary.<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Your job is to produce a final document divided in chapters and bullet points.\n",
      "You are given a text containing an existing summary to a certain point:\n",
      "\n",
      " training\n",
      "\n",
      "Sure, here's the refined summary with the additional context:\n",
      "\n",
      "**Training**\n",
      "\n",
      "* The model is trained on one fold with a random split (8 folds in total) or the full dataset using the best parameters Onecycle scheduler with 0.1 warmup. Use weighted CrossEntropyLoss. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat) Implement a hypercolumn for EfficientNet with 5 blocks.\n",
      "* Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule. A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained. A 3-layer model was initialized with 4-layer model's first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.\n",
      "\n",
      "**Additional Details**\n",
      "\n",
      "* Random affine: Same as @hengck23 shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).  \n",
      "* Random interpolation: Slightly scale and shift the time dimension.  \n",
      "* Flip pose: Flip the x-coordinates of all points. In CNN, x_new = x_max - x_old. In transformer, x_new = 2 * frame[:,0,0] - x_old.\n",
      "\n",
      "You can now refine it (if necessary) with more context below.\n",
      "\n",
      "4. Submissions, Conversion and Ensemble\n",
      "We rewrote all our models in Keras and transferred PyTorch weights to them, resulting in a speed boost of around 30%. For transformer model, pytorch-onnx-tf-tflite will generate too much useless tensor shape operations, a fully rewriting can reduce these manually. For CNN model, we rewrote DepthwiseConv2D with a hard-coded way, whose speed is 200%~300% of its original version of tflite DepthwiseConv2D.  \n",
      "After that, we aggregated all these models in the tf.Module class. Converting directly from Keras resulted in lower speed (don't know why).  \n",
      "We calculated ensemble weights for models trained on fold 0 using the local fold 0 score and applied these weights to the full dataset models.  \n",
      "EfficientNet-B0 achieved a leaderboard score of approximately 0.8, and transformers improved the score to 0.81. The final ensemble included:  \n",
      "Efficientnet-B0, fold 0 BERT, full data train DeBERTa, full data train  \n",
      "Interestingly, a key feature was using the ensemble without softmax, which consistently provided a boost of around 0.01.\n",
      "\n",
      "Given the new context, refine the original summary.<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m<bos><start_of_turn>user\n",
      "Your job is to produce a final document divided in chapters and bullet points.\n",
      "You are given a text containing an existing summary to a certain point:\n",
      "\n",
      " training\n",
      "\n",
      "Sure, here's the refined summary with the additional context:\n",
      "\n",
      "**Training**\n",
      "\n",
      "* The model is trained on one fold with a random split (8 folds in total) or the full dataset using the best parameters Onecycle scheduler with 0.1 warmup. Use weighted CrossEntropyLoss. Increase the weights for poorly predicted classes and classes with semantically similar pairs (such as kitty and cat) Implement a hypercolumn for EfficientNet with 5 blocks.\n",
      "* Train on one fold with a random split (8 folds in total) or the full dataset using the best parameters Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule. A 4-layer, 256 hidden-size, 512 intermediate-size transformer were trained. A 3-layer model was initialized with 4-layer model's first 3 layers. Knowledge distillation were used in 3-layer model training, in which the 4-layer model is the teacher.\n",
      "* Additional details:\n",
      "    * Random affine: Same as @hengck23 shared. In CNN, after global affine, shift-scale-rotate was also applied to each part separately (e.g. hand, lip, body-pose).\n",
      "    * Random interpolation: Slightly scale and shift the time dimension.\n",
      "    * Flip pose: Flip the x-coordinates of all points.\n",
      "\n",
      "You can now refine it (if necessary) with more context below.\n",
      "\n",
      "5. PS. Need BETTER TFlite DepthwiseConv2D\n",
      "Depthwise convolution models performed very well for these tasks, outperforming other CNN and ViT models (rexnet_100 was also good). We spent a lot of time dealing with the conversion of DepthwiseConv2D operation. Here are some strange results:  \n",
      "Given a input image with 82x42x32 (HWC), there are two ways to do a 3x3 depthwise convolution in Keras. One is Conv2D(32, 3, groups = 32), the other is DepthwiseConv2D(3). However, after converting these two to tflite, the running time of the Conv2D is 5.05ms, and the running time of DepthwiseConv2D is 3.70ms. More strangely, a full convolution Conv2D(32, 3, groups = 1) with FLOPs = HWC^2 only takes 2.09ms, even faster than previous two with FLOPs = HWC.  \n",
      "Then we rewrote the depthwise-conv like this:  \n",
      "The running time of this is 1.24 ms.  \n",
      "In summary, our version (1.24ms) > full Conv2D with larger FLOPs (2.09ms) > DepthwiseConv2D (3.70ms) > Conv2D(C, groups = C) (5.05ms).  \n",
      "However, our version introduced too much nodes in tflite graph, which is not stable in running time. If the tensorflow team has a better implementation of DepthwiseConv2D, we can even ensemble two CNN models, which is expected to reach 0.82 LB.  \n",
      "By the way, EfficientNet with ONNX was ~5 times faster than TFLite.  \n",
      "github code\n",
      "\n",
      "Given the new context, refine the original summary.<end_of_turn>\n",
      "<start_of_turn>model\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " training details:\n",
      "\n",
      "* **Training split:** 8 folds with a random split.\n",
      "* **Optimizer:**\n",
      "    * Onecycle scheduler with 0.1 warmup.\n",
      "    * Ranger optimizer with 60% flat and 40% cosine annealing learning rate schedule.\n",
      "* **Model architecture:**\n",
      "    * 4-layer, 256 hidden-size, 512 intermediate-size transformer.\n",
      "    * 3-layer model initialized from 4-layer model's first 3 layers.\n",
      "    * Knowledge distillation from 4-layer model to 3-layer model.\n",
      "* **Depthwise convolution:**\n",
      "    * Used for specific tasks and yielded better results than other models.\n",
      "    * Conversion between DepthwiseConv2D and Conv2D was explored and compared.\n",
      "    * A version with 1 group was found to be faster than the full Conv2D.\n",
      "* **Performance:**\n",
      "    * EfficientNet with ONNX was ~5 times faster than TFLite.\n",
      "    * Full Conv2D with larger FLOPs took 2.09ms, while DepthwiseConv2D took 3.70ms.\n",
      "    * Our version of DepthwiseConv2D took 1.24ms, which is faster than the full Conv2D but slower than the other versions.\n",
      "</start_of_turn>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define prompt for the first summarization\n",
    "prompt_template = \"\"\"<bos><start_of_turn>user\n",
    "Summarize the following text in a technical way. Focus on facts, numbers and strategies used. Divide the summary in chapters, be impersonal and use bullet points:\n",
    "\n",
    "{text}<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "prompt_init = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# Define prompt for the refine phase, enhancing the previous summary with the new information\n",
    "refine_template = \"\"\"<bos><start_of_turn>user\n",
    "Your job is to produce a final document divided in chapters and bullet points.\n",
    "You are given a text containing an existing summary to a certain point:\n",
    "\n",
    "{existing_answer}\n",
    "\n",
    "You can now refine it (if necessary) with more context below.\n",
    "\n",
    "{text}\n",
    "\n",
    "Given the new context, refine the original summary.<end_of_turn>\n",
    "<start_of_turn>model\"\"\"\n",
    "prompt_refine = PromptTemplate.from_template(refine_template)\n",
    "\n",
    "\n",
    "chain = load_summarize_chain(langchain_hf, chain_type='refine',\n",
    "                             return_intermediate_steps=True,\n",
    "                             input_key='input_documents',\n",
    "                             output_key='output_text',\n",
    "                             question_prompt=prompt_init,\n",
    "                             refine_prompt=prompt_refine,\n",
    "                             verbose=True)\n",
    "\n",
    "out_summary = chain.invoke(splits, return_only_outputs=True)\n",
    "print(out_summary['output_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Gemma with LoRa¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>highlights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sally Forrest, an actress-dancer who graced th...</td>\n",
       "      <td>Sally Forrest, an actress-dancer who graced th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A middle-school teacher in China has inked hun...</td>\n",
       "      <td>Works include pictures of Presidential Palace ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A man convicted of killing the father and sist...</td>\n",
       "      <td>Iftekhar Murtaza, 29, was convicted a year ago...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avid rugby fan Prince Harry could barely watch...</td>\n",
       "      <td>Prince Harry in attendance for England's crunc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Triple M Radio producer has been inundated w...</td>\n",
       "      <td>Nick Slater's colleagues uploaded a picture to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  \\\n",
       "0  Sally Forrest, an actress-dancer who graced th...   \n",
       "1  A middle-school teacher in China has inked hun...   \n",
       "2  A man convicted of killing the father and sist...   \n",
       "3  Avid rugby fan Prince Harry could barely watch...   \n",
       "4  A Triple M Radio producer has been inundated w...   \n",
       "\n",
       "                                          highlights  \n",
       "0  Sally Forrest, an actress-dancer who graced th...  \n",
       "1  Works include pictures of Presidential Palace ...  \n",
       "2  Iftekhar Murtaza, 29, was convicted a year ago...  \n",
       "3  Prince Harry in attendance for England's crunc...  \n",
       "4  Nick Slater's colleagues uploaded a picture to...  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import of the validation set which contains fewer examples than training\n",
    "validation = pd.read_csv('cnn_dailymail/validation.csv')[['article', 'highlights']]\n",
    "validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "model = \"google_model/gemma_2b_it\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=6,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "tokenizer.padding_side = \"right\" # Fixing overflow issue ref: source code\n",
    "model = AutoModelForCausalLM.from_pretrained(model, device_map=\"auto\", quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_of_turn>user\n",
      "Given the following article, write a short summary of the article in 2-3 sentences:\n",
      "\n",
      "Article: Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films such as the 1956 noir While the City Sleeps died on March 15 at her home in Beverly Hills, California. Forrest, whose birth name was Katherine Feeney, was 86 and had long battled cancer. Her publicist, Judith Goffin, announced the news Thursday. Scroll down for video . Actress: Sally Forrest was in the 1951 Ida Lupino-directed film 'Hard, Fast and Beautiful' (left) and the 1956 Fritz Lang movie 'While the City Sleeps' A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films including the critical and commercial success Not Wanted, Never Fear and Hard, Fast and Beautiful. Some of Forrest's other film credits included Bannerline, Son of Sinbad, and Excuse My Dust, according to her iMDB page. The page also indicates Forrest was in multiple Climax! and Rawhide television episodes. Forrest appeared as herself in an episode of The Ed Sullivan Show and three episodes of The Dinah Shore Chevy Show, her iMDB page says. She also starred in a Broadway production of The Seven Year Itch. City News Service reported that other stage credits included As You Like It, No, No, Nanette and Damn Yankees. Forrest married writer-producer Milo Frank in 1951. He died in 2004. She is survived by her niece, Sharon Durham, and nephews, Michael and Mark Feeney. Career: A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Sally Forrest, an actress-dancer who graced the silver screen throughout the '40s and '50s in MGM musicals and films died on March 15 .\n",
      "Forrest, whose birth name was Katherine Feeney, had long battled cancer .\n",
      "A San Diego native, Forrest became a protege of Hollywood trailblazer Ida Lupino, who cast her in starring roles in films .<end_of_turn>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data = Dataset.from_pandas(validation)\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['article'])):\n",
    "        messages = [\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": \"Given the following article, write a short summary of the article in 2-3 sentences:\\n\\nArticle: {}\".format(example['article'][i])},\n",
    "            {\"role\": \"assistant\",\n",
    "             \"content\": \"{}\".format(example['highlights'][i])}\n",
    "        ]\n",
    "        output_texts.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False))\n",
    "        \n",
    "    return output_texts\n",
    "\n",
    "# Print the first training example\n",
    "print(formatting_prompts_func(train_data[:1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 13368/13368 [00:04<00:00, 3146.53 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:23, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.549100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.083900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.099100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.627300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.699800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.631100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.365700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.536800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.640500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.586300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25, training_loss=2.80006965637207, metrics={'train_runtime': 24.1194, 'train_samples_per_second': 4.146, 'train_steps_per_second': 1.037, 'total_flos': 599516784721920.0, 'train_loss': 2.80006965637207, 'epoch': 0.01})"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    max_seq_length=512,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=25,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=2,\n",
    "        report_to='none',\n",
    "        output_dir='logs',\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
